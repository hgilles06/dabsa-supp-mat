{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xgboost\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor \n",
    "from sklearn.inspection import plot_partial_dependence, partial_dependence\n",
    "\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    " \n",
    "import shap\n",
    "from scipy.linalg import toeplitz\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "n=10000\n",
    "k = 6\n",
    "rho = 0.6\n",
    "cova = rho**toeplitz(range(6), range(6))\n",
    "\n",
    "X =  np.random.multivariate_normal(mean=[0]*k, cov=cova, size=n)  ### TFIDF\n",
    "beta = np.random.uniform(low=-1.0, high=1.0, size=(k,1))\n",
    "beta = beta/np.abs(beta).sum()\n",
    "eps = np.random.normal(loc=0.0, scale=0.1, size=(n,1))\n",
    "\n",
    "f = [lambda x:x, lambda x:x**2, lambda x: np.sin(x), lambda x: np.exp(x), lambda x: np.abs(np.log(abs(x))),\n",
    "     lambda x: x*(x>0)]\n",
    "\n",
    "#y = beta[0] * X[:,0] + beta + eps\n",
    "\n",
    "y = np.zeros((n,1))\n",
    "for j in range(k):\n",
    "    \n",
    "    y +=  (beta[j] * f[j](X[:,j])).reshape((n,1))\n",
    "    \n",
    "        \n",
    "y = y + eps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = 3,2\n",
    "fig, ax = plt.subplots(nrows=nrows,ncols=ncols, figsize=(20,12))\n",
    "\n",
    "line=0\n",
    "col =0\n",
    "pos = 0\n",
    "\n",
    "\n",
    "for var in range(k):\n",
    "    \n",
    "    ax[line,col].plot(X[:,var], beta[var] * f[var](X[:,var]), \".\")\n",
    "    ax.flat[pos].label_outer()\n",
    "    \n",
    "    if ((col+1)%ncols==0):\n",
    "        line+=1\n",
    "        col=0\n",
    "\n",
    "    else:\n",
    "        col+=1\n",
    "\n",
    "    pos+=1\n",
    "\n",
    "    \n",
    "plt.rc('text', usetex=True)\n",
    "for i,ax in enumerate(fig.axes):\n",
    "    plt.sca(ax)\n",
    "    plt.xlabel(r\"$X_%s$\"%(i+1), fontsize=16)\n",
    "    plt.ylabel(r\"$\\beta_%sf_%s(X_%s)$\"%(i+1,i+1,i+1), fontsize=16)\n",
    "    \n",
    "    \n",
    "plt.savefig(\"C:/Users/hgill/Documents/Etudes/DABSA/Redaction/pics/betaf.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Shap values with the Gradient Boosting algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = xgboost.train(params={\"learning_rate\": 0.01}, dtrain=xgboost.DMatrix(X, label=y), num_boost_round=1000)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows, ncols = 3,2\n",
    "fig, ax = plt.subplots(nrows=nrows,ncols=ncols, figsize=(20,12))\n",
    "\n",
    "line=0\n",
    "col =0\n",
    "pos = 0\n",
    "\n",
    "\n",
    "for var in range(k):\n",
    "    \n",
    "    ax[line,col].plot(X[:,var], shap_values[:,var], \".\")\n",
    "    ax.flat[pos].label_outer()\n",
    "    \n",
    "    if ((col+1)%ncols==0):\n",
    "        line+=1\n",
    "        col=0\n",
    "\n",
    "    else:\n",
    "        col+=1\n",
    "\n",
    "    pos+=1\n",
    "\n",
    "    \n",
    "plt.rc('text', usetex=True)\n",
    "for i,ax in enumerate(fig.axes):\n",
    "    plt.sca(ax)\n",
    "    plt.xlabel(r\"$X_%s$\"%(i+1), fontsize=16)\n",
    "    plt.ylabel(r\"$\\hat{\\beta_%s}\\hat{f}_%s(X_%s)$\"%(i+1,i+1,i+1), fontsize=16)\n",
    "\n",
    "plt.savefig(\"C:/Users/hgill/Documents/Etudes/DABSA/Redaction/pics/betaf_hat.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(xgboost.DMatrix(X)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = LinearRegression()\n",
    "\n",
    "linear.fit(shap_values.sum(axis=1).reshape(-1,1), y)\n",
    "\n",
    "beta_hat = linear.coef_.ravel()[0]\n",
    "intercept = linear.intercept_[0]\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(shap_values.sum(axis=1).reshape(-1,1), y, \".\")\n",
    "plt.xlabel(r\"$\\displaystyle \\sum_{j=1}^{6}\\hat{\\phi_j}$\", fontsize=16)\n",
    "plt.ylabel(r\"$y$\", fontsize=16)\n",
    "\n",
    "plt.text(8,0, r\" $\\hat{y} = %s + %s \\displaystyle \\sum_{j=1}^{6}\\hat{\\phi_j}$\"%(round(intercept,2), round(beta_hat,2)), fontsize=16)\n",
    "plt.savefig(\"C:/Users/hgill/Documents/Etudes/DABSA/Redaction/pics/ySumPhi_hat.png\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
