{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import warnings\n",
    "\n",
    "import spacy\n",
    "from spacy_lefff import LefffLemmatizer, POSTagger\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint\n",
    "from gensim.models import CoherenceModel\n",
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import os\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "\n",
    "# Load the library with the CountVectorizer method\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "from threading import Thread, RLock\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "article_df = pd.read_pickle(\"data//articles\")\n",
    "article_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "article_df = article_df.drop_duplicates()\n",
    "article_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting randomly 50.000 articles to reduce the computation time for the LDA process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "df = article_df.sample(50000, replace=False).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.article = df.article.apply(lambda text: re.sub('[\",\\.!?]', '', text))\n",
    "df.article = df.article.str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "french_stopwords =  stopwords.words('french') + [\"plus\", \"cette\", \"comme\", \"depuis\", \"être\", \"fait\", \"deux\", \"entre\", \"aussi\", \n",
    "                                                \"si\", \"ans\",\"tout\", \"après\", \"faire\", \"sans\", \"bien\", \"très\", \"leurs\", \"où\", \"dont\",\n",
    "                                               \"contre\", \"selon\",\"encore\",\"moins\", \"alors\",\"premier\",\"peut\",\"000\", \n",
    "                                               \"tous\", \"toutes\",\"années\", \"année\", \"mois\", \"autres\", \"avant\", \"après\", \"avoir\",\"non\", \"autre\",\"peu\",\n",
    "                                                 \"un\", \"deux\",\"trois\", \"quatre\",\"cinq\",\"sous\"\n",
    "                                                \"ainsi\", \"fois\", \"fin\",  \"aujourd\", \"hui\", \"ainsi\",\"déjà\", \"cela\", \"dit\", \"quelques\", \"toujours\", \"lors\", \"faut\", \n",
    "                                                 \"jusqu\", \"plusieurs\", \"donc\", \"là\", \"doit\", \"celui\", \"quand\", \"elles\", \"cas\", \"ceux\",\"va\", \"cet\",\"celle\", \"celles\", \n",
    "                                                \"après\", \"était\", \"être\", \"été\", \"même\", \"très\", \"ca\", \"dire\", \"ni\", \n",
    "                                                \"sous\", \"vers\", \"ici\", \"car\", \"trop\", \"beaucoup\", \"grand\", \"père\", \"dernier\",\"devant\", \"près\", \"heures\", \"jour\", \"jours\", \"chaque\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#Most common words\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "# Helper function\n",
    "def plot_most_common_words(count_data, count_vectorizer, n=10, save_path=\"Redaction/pics/most_common_words.png\"):\n",
    "    \n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts+=t.toarray()[0]\n",
    "    \n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:n]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words)) \n",
    "    \n",
    "    plt.figure(2, figsize=(15, 15/1.6180))\n",
    "    plt.subplot(title='%s most common words'%n)\n",
    "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
    "    sns.barplot(x_pos, counts, palette='husl')\n",
    "    plt.xticks(x_pos, words, rotation=60) \n",
    "    plt.xlabel('words')\n",
    "    plt.ylabel('counts')\n",
    "    plt.savefig(save_path)\n",
    "    plt.show()\n",
    "    \n",
    "# Initialise the count vectorizer with the french stop words\n",
    "count_vectorizer = CountVectorizer(stop_words=french_stopwords)\n",
    "# Fit and transform the processed text\n",
    "count_data = count_vectorizer.fit_transform(df.article)\n",
    "# Visualise the 40 most common words\n",
    "plot_most_common_words(count_data, count_vectorizer, n=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "df[\"preprocessed_article\"] = df.article.apply(lambda article: [ word for word in simple_preprocess(article) if word not in french_stopwords])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Modeling: Bi-grams and Tri-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(df.preprocessed_article, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df[\"bigram_article\"] = df.preprocessed_article.apply(lambda article: bigram_mod[article])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_pickle(\"data/sampleLDA_preprocessed_articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating word dictionary and corpus (using words and bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_pickle(\"data/sampleLDA_preprocessed_articles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(df.bigram_article)\n",
    "\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(article) for article in df.bigram_article]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal number of topics for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1, iterations=100):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    os.environ.update({'MALLET_HOME':r'C:/Users/hgill/Documents/Etudes/DABSA/data/mallet-2.0.8/'}) \n",
    "    mallet_path = r'C:/Users/hgill/Documents/Etudes/DABSA/data/mallet-2.0.8/bin/mallet'\n",
    "    \n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word, iterations=iterations)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=df.bigram_article, start=2, limit=20, step=2, iterations=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "file_path = \"model_list_coherence_values.pkl\"\n",
    "\n",
    "with open(file_path, 'ab') as dbfile: \n",
    "    pickle.dump((model_list, coherence_values), dbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "# Show graph\n",
    "limit=20; start=2; step=2;\n",
    "x = range(start, limit, step)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Number of topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "#plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.savefig(\"Redaction/pics/coherence_score.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Mallet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mallet_home = path_to_mallet #you have to replace it by the path to the mallet directory\n",
    "os.environ.update({'MALLET_HOME':mallet_home}) \n",
    "mallet_path = os.path.join(mallet_home, \"/bin/mallet\")\n",
    "\n",
    "num_topics = 14\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus , num_topics=num_topics, id2word=id2word, iterations=1000, workers=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model\n",
    "\n",
    "file_path = \"ldamallet.pkl\"\n",
    "\n",
    "with open(file_path, 'ab') as dbfile: \n",
    "    pickle.dump(ldamallet, dbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "num_topics = 14\n",
    "\n",
    "# Show Topics\n",
    "pprint(ldamallet.show_topics(num_topics=num_topics, formatted=False))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=df.bigram_article, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aspects = ldamallet.show_topics(num_topics = num_topics, formatted=False, num_words=100)\n",
    "aspects_df = pd.DataFrame(aspects, columns=[\"aspects\", \"words_imp\"])\n",
    "aspects_df[\"words\"] = aspects_df.words_imp.apply(lambda words_imp: [word_imp[0] for word_imp in words_imp])\n",
    "aspects_df[\"imp\"] = aspects_df.words_imp.apply(lambda words_imp: [word_imp[1] for word_imp in words_imp])\n",
    "#aspects_df[\"most_imp_words\"] = aspects_df.words_imp.apply(lambda word_imp: sorted([(imp,word) for word,imp in word_imp],reverse=True)[0][1])\n",
    "aspects_df[\"nwords\"] = aspects_df.words.apply(lambda words:len(words))\n",
    "\n",
    "aspects_df = aspects_df.sort_values(by=[\"aspects\"], axis=0).reset_index(drop=True)\n",
    "aspects_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_name(words):\n",
    "\n",
    "    if (\"américain\" in words)&(\"etats_unis\" in words):\n",
    "        return \"United States\"\n",
    "    if (\"gouvernement\" in words)&(\"négociations\" in words):\n",
    "        return \"government\"\n",
    "    if \"film\" in words:\n",
    "        return \"culture\"\n",
    "    if (\"france\" in words)&(\"équipe\" in words):\n",
    "        return \"sport\"\n",
    "    if (\"politique\" in words)&(\"ps\" in words):\n",
    "        return \"politics\"\n",
    "    if (\"vie\" in words)&(\"monde\" in words):\n",
    "        return \"society\"\n",
    "    if (\"travail\" in words)&(\"jeunes\" in words):\n",
    "        return \"employment\"\n",
    "    if (\"justice\" in words)&(\"loi\" in words):\n",
    "        return \"justice\"\n",
    "    if (\"santé\" in words)&(\"eau\" in words):\n",
    "        return \"health\"\n",
    "    if (\"groupe\" in words)&(\"marché\" in words):\n",
    "        return \"firms\"\n",
    "    if (\"politique\" in words)&(\"europe\" in words):\n",
    "        return \"Europe\"\n",
    "    if (\"télévision\" in words)&(\"internet\" in words):\n",
    "        return \"media\"\n",
    "    if (\"économie\" in words)&(\"croissance\" in words):\n",
    "        return \"growth\"\n",
    "    if (\"ville\" in words)&(\"personnes\" in words):\n",
    "        return \"cities\"\n",
    "    \n",
    "    return words[0]\n",
    "    \n",
    "aspects_df[\"aspect_name\"] = aspects_df.words.apply(topic_name)\n",
    "aspects_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the aspect dataframe\n",
    "aspects_df.to_csv(\"data/aspects.csv\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%time\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "aspect_list = aspects_df.aspect_name.values\n",
    "nrows=4\n",
    "ncols=4\n",
    "\n",
    "fig, ax = plt.subplots(nrows=nrows,ncols=ncols, figsize=(14,18))\n",
    "\n",
    "line=0\n",
    "col =0\n",
    "pos = 0\n",
    "\n",
    "for i,aspect in enumerate(aspect_list):\n",
    "    \n",
    "    \n",
    "    if pos==12:\n",
    "        col+=1\n",
    "        pos+=1\n",
    "        \n",
    "    text = \" \".join(aspects_df.words[i])\n",
    "\n",
    "    wc = WordCloud(random_state=2,\n",
    "                   relative_scaling=0.2, min_font_size =10 ,background_color='white', width=1000, height=1000)\n",
    "\n",
    "    frequences = dict(aspects_df.words_imp[i] )\n",
    "    wc.generate_from_frequencies(frequences)\n",
    "    \n",
    "    \n",
    "    ax[line,col].imshow(wc, interpolation='bilinear')\n",
    "\n",
    "    ax.flat[pos].set_title(aspects_df.aspect_name[i])\n",
    "    \n",
    "    ax.flat[pos].label_outer()\n",
    "    \n",
    "    \n",
    "    if ((col+1)%ncols==0):\n",
    "        line+=1\n",
    "        col=0\n",
    "\n",
    "    else:\n",
    "        col+=1\n",
    "\n",
    "    pos+=1\n",
    "\n",
    "    \n",
    "for ax in fig.axes:\n",
    "    plt.sca(ax)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.savefig(\"pics/aspects.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
